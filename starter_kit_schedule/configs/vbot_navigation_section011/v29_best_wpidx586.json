{
  "run_tag": "v29_best_wpidx586",
  "env_name": "vbot_navigation_section011",
  "starter_kit_dir": "D:\\MotrixLab\\starter_kit\\navigation2",
  "best_checkpoint": {
    "run": "26-02-17_10-45-19-130319_PPO",
    "file": "agent_6000.pt",
    "metric_wp_idx": 5.8643,
    "schedule_path": "starter_kit_schedule/checkpoints/vbot_navigation_section011/best_agent.pt"
  },
  "description": "v29 boosted later-phase bonuses. Warm-started from v28 agent_4000.pt. LR=1e-4 (0.25x original) for ultra-stable fine-tuning. Peak wp_idx=5.86 at iter 6000, degradation after (catastrophic forgetting). 3-jump celebration system (v27), ordered zone targeting (v25), gradient-only architecture (v23b).",
  "reward_scales": {
    "forward_velocity": 2.875,
    "waypoint_approach": 166.5,
    "waypoint_facing": 0.061,
    "position_tracking": 0.384,
    "alive_bonus": 1.446,
    "waypoint_bonus": 50.0,
    "smiley_bonus": 20.0,
    "red_packet_bonus": 20.0,
    "phase_completion_bonus": 25.0,
    "zone_approach": 35.06,
    "height_progress": 28.30,
    "height_approach": 5.0,
    "height_oscillation": -2.0,
    "traversal_bonus": 0.0,
    "foot_clearance": 0.053,
    "foot_clearance_bump_boost": 4.39,
    "jump_reward": 10.0,
    "per_jump_bonus": 25.0,
    "celebration_bonus": 80.0,
    "stance_ratio": 0.041,
    "vel_heading_alignment": 0.0,
    "heading_command_shaping": 0.0,
    "swing_contact_penalty": -0.031,
    "swing_contact_bump_scale": 0.356,
    "impact_penalty": -0.080,
    "torque_saturation": -0.025,
    "orientation": -0.027,
    "slope_orientation": 0.0,
    "lin_vel_z": -0.195,
    "ang_vel_xy": -0.045,
    "torques": -5e-6,
    "dof_vel": -3e-5,
    "dof_acc": -1.5e-7,
    "action_rate": -0.008,
    "termination": -200.0
  },
  "rl_overrides": {
    "learning_rate": 1.0e-4,
    "learning_rate_original": 4.24e-4,
    "learning_rate_note": "1e-4 is 0.25x original for warm-start fine-tuning. Use 4.24e-4 for fresh training.",
    "entropy_loss_scale": 4.11e-3,
    "policy_hidden_layer_sizes": [256, 128, 64],
    "value_hidden_layer_sizes": [512, 256, 128],
    "rollouts": 24,
    "learning_epochs": 6,
    "mini_batches": 16,
    "discount_factor": 0.999,
    "lambda_param": 0.99,
    "max_env_steps": 50000000,
    "seed": 42,
    "check_point_interval": 1000,
    "num_envs": 2048,
    "grad_norm_clip": 1.0,
    "ratio_clip": 0.2,
    "value_clip": 0.2
  },
  "env_overrides": {
    "action_scale": 0.6,
    "max_episode_seconds": 60.0,
    "max_episode_steps": 6000,
    "grace_period_steps": 100,
    "celebration_jump_threshold": 1.55,
    "required_jumps": 3,
    "celebration_landing_z": 1.50
  },
  "training_history": {
    "warm_start_chain": [
      "Stage 15 (gamma/lambda optimized, wp_idx=1.977)",
      "-> v23b-T7 AutoML winner (gradient-only, wp_idx=0.582@5M)",
      "-> v25 ordered targeting",
      "-> v27 multi-jump celebration",
      "-> v28 re-enabled bonuses (wp_idx=5.05)",
      "-> v29 boosted later-phase bonuses (wp_idx=5.86 BEST)"
    ],
    "key_discoveries": [
      "gamma=0.999 + lambda=0.99: single biggest breakthrough (Stages 11-15)",
      "Gradient-only architecture (v23b): remove all discrete bonuses, train with distance gradients",
      "Re-enabling bonuses (v28-v29): after gradient-trained policy, adding back bonuses boosts late-phase collection",
      "Ultra-low LR (0.25x) for warm-start prevents catastrophic forgetting",
      "Performance peaks at iter 4000-6000 then degrades â€” checkpoint early"
    ]
  }
}
