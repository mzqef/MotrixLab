---
name: reward-penalty-engineering
description: Methodology for exploring, testing, and archiving reward/penalty functions for VBot quadruped navigation. A process-oriented guide for systematic reward discovery.
---

## Purpose

This skill teaches the **methodology of reward/penalty exploration** â€” how to discover, test, evaluate, and archive reward signals. It is a process guide, not a recipe book.

- **How to identify** what reward/penalty to try next
- **How to formulate** a hypothesis and test it
- **How to evaluate** whether a reward change helped
- **How to archive** findings in the reward library for reuse

> **IMPORTANT:**
> - The reward function lives in `starter_kit/navigation*/vbot/vbot_*_np.py` â†’ `_compute_reward()`.
> - Reward weights are in `starter_kit/navigation*/vbot/cfg.py` â†’ `RewardConfig.scales` dict.
> - The reward function IS fully implemented with: position tracking (sigma=5.0), fine position tracking (sigma=0.3, threshold=1.5m), heading, forward velocity, distance_progress (linear), approach, arrival bonus (50), stop bonus, alive_bonus (conditional on NOT reached), time_decay, stability penalties, and termination (-100).
> - Anti-laziness mechanisms (conditional alive_bonus, time_decay, successful truncation) are active. Do NOT remove them.
> - Do NOT re-implement the reward function from scratch. Modify weights or add new terms incrementally.

> This skill does NOT contain reward component examples or scale tables.
> Those live in their respective locations:
>
> | What | Where |
> |------|-------|
> | Component reference & scale ranges | `starter_kit_schedule/templates/reward_config_template.yaml` |
> | Archived reward/penalty instances | `starter_kit_schedule/reward_library/` |
> | Terrain strategies & reward code | `quadruped-competition-tutor` skill |
> | Stage-specific reward overrides | `curriculum-learning` skill |
> | Reward weight search spaces | `hyperparameter-optimization` skill |
> | Visual reward debugging | `subagent-copilot-cli` skill |

---

## When to Use This Skill

| Situation | Use This |
|-----------|----------|
| "I need a new reward idea" | âœ… Follow the Discovery Process |
| "This reward isn't working, what now?" | âœ… Follow Diagnostic Methodology |
| "I want to compare two reward designs" | âœ… Follow Experiment Protocol |
| "I found a good reward, where to save it?" | âœ… Follow Archiving Process |
| "What are the reward scale ranges?" | âŒ Read `reward_config_template.yaml` |
| "What reward code exists for stairs?" | âŒ Read `quadruped-competition-tutor` |
| "How do I tune reward weights automatically?" | âŒ Read `hyperparameter-optimization` |

---

## The Exploration Cycle

Reward engineering is iterative. Every change follows this cycle:

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   DIAGNOSE   â”‚ â† What behavior is wrong?
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  HYPOTHESIZE  â”‚ â† What reward signal could fix it?
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   IMPLEMENT   â”‚ â† Minimal change, one variable at a time
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     TEST      â”‚ â† Short run (1-2M steps), multiple seeds
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   EVALUATE    â”‚ â† Did the hypothesis hold?
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   ARCHIVE     â”‚ â† Record result in reward library
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
      Next cycle
```

**Rule:** Never change more than one reward dimension per cycle. If you change both the termination penalty AND add a new gait reward, you cannot attribute outcomes.

---

## Phase 1: Diagnose

### Behavioral Signals

Before touching rewards, identify **what behavior** is wrong. Not "the reward is too low" but a concrete observable:

| Observable | Likely Reward Gap |
|------------|-------------------|
| Robot doesn't move | Missing or weak positive incentive |
| Robot moves but falls | Missing or weak stability penalty |
| Robot oscillates near goal | Reward gradient too steep near target |
| Robot takes bizarre paths | Reward hacking â€” high reward from unintended behavior |
| Robot crouches/crawls | Missing height maintenance signal |
| Robot ignores obstacles | Missing proximity/collision signal |
| Robot is fast but jerky | Missing smoothness penalty |
| Robot is stable but slow | Positive incentive too weak relative to penalties |
| Reward curve plateaus | Reward provides no gradient in current state region |
| **Robot stands still near target** | **alive_bonus accumulation > goal reward â€” see Lazy Robot Case Study below** |
| **Distance increases during training** | **Reward hacking via per-step bonus. Check alive_bonus Ã— avg_ep_len vs arrival_bonus** |
| **Episode length near max, reached% drops** | **Robot exploiting per-step rewards instead of completing task** |

### Diagnostic Commands

```powershell
# 1. Watch the policy â€” ALWAYS start here before looking at numbers
uv run scripts/play.py --env vbot_navigation_section001

# 2. Train with rendering to see behavior in real time
uv run scripts/train.py --env vbot_navigation_section001 --render

# 3. TensorBoard for reward curves
uv run tensorboard --logdir runs/vbot_navigation_section001
```

### Visual Diagnosis

Use `subagent-copilot-cli` to analyze simulation frames and training curves:

```powershell
# Describe what you see, ask what reward signal is missing
copilot --model gpt-4.1 --allow-all -p "Watch this simulation frame. The robot is <describe behavior>. What reward signal might cause this?" -s
```

> **Key insight:** A reward signal is "missing" if the agent has no gradient pointing toward the desired behavior in its current state. The fix may be a new reward, a penalty, or reshaping an existing one.

---

## Phase 2: Hypothesize

### Formulating a Good Hypothesis

A testable reward hypothesis has three parts:

1. **Behavior target:** What the robot should do differently
2. **Signal mechanism:** What mathematical signal encodes that behavior
3. **Expected side effect:** What might go wrong

**Template:**

> "If I add/modify `<signal>` with weight `<w>`, the robot should `<desired behavior>`, but might also `<risk>`."

### Discovery Strategies

When you don't know what to try, use these strategies to generate candidates:

#### Strategy 1: Inversion

Take the undesired behavior and directly penalize it.

> Robot bouncing â†’ penalize vertical velocity
> Robot spinning â†’ penalize angular velocity
> Robot retreating â†’ penalize backward displacement

#### Strategy 2: Shaping the Gradient

If the robot is stuck, the reward surface is flat in its current region. Add a signal that creates local gradient:

> Robot stuck far from goal â†’ Add distance-based shaping (sigmoid, exponential)
> Robot stuck near goal â†’ Add fine-grained proximity bonus
> Robot stuck on terrain edge â†’ Add progress checkpoints

#### Strategy 3: Proxy Decomposition

Break the competition score into component sub-goals and create a signal for each:

> Final score = traversal + bonus zones + time bonus
> â†’ Create separate signals for: forward progress, zone proximity, speed

#### Strategy 4: Biomimetic Analogy

What would a real quadruped "want" in this situation?

> Stairs â†’ lift knees higher
> Uneven ground â†’ keep center of mass low
> Obstacles â†’ slow down, increase awareness

#### Strategy 5: Ablation Discovery

Temporarily remove one existing reward and see what degrades:

```powershell
# Remove component to see its effect
python scripts/train.py --env <env> --seed 42 --cfg-override "reward_config.scales.<component>=0.0"
```

If removing a component doesn't change behavior, it was irrelevant. If behavior collapses, it was critical.

#### Strategy 6: Competition-Score Alignment

Compare training reward to competition scoring rules. Gaps indicate missing signals:

> Competition awards points for stopping in smiley zones
> â†’ but training reward only rewards forward velocity
> â†’ mismatch: need a "stop in zone" signal

> **Refer to** `quadruped-competition-tutor` skill for competition scoring rules.

#### Strategy 7: Browse the Library

Check previously tried components in the reward library before inventing new ones:

```powershell
# Browse archived reward components
Get-ChildItem starter_kit_schedule/reward_library/components/ | Select-Object Name
# Read a specific component's notes
Get-Content starter_kit_schedule/reward_library/components/<name>.yaml
```

---

## Phase 3: Implement

### Principles

1. **One variable at a time** â€” Change a single reward component per experiment
2. **Minimal change** â€” Prefer adjusting a weight before adding new code
3. **Use existing infrastructure** â€” Check `reward_config_template.yaml` for components that can be enabled/disabled before writing new code

### Where to Make Changes

| Change Type | Location |
|-------------|----------|
| Adjust existing weight | `starter_kit/navigation1/vbot/cfg.py` â†’ `RewardConfig.scales` dict |
| Add new reward term | `starter_kit/navigation1/vbot/vbot_section001_np.py` â†’ `_compute_reward()` |
| Configure component | `starter_kit_schedule/templates/reward_config_template.yaml` |

### Change Magnitude Guidelines

When adjusting weights, use **multiplicative steps** not additive:

- **Small adjustment:** Ã—0.5 or Ã—2 (halve or double)
- **Medium adjustment:** Ã—0.1 or Ã—10
- **Large adjustment:** Ã—0.01 or Ã—100

For new components, start with a weight that produces reward magnitude comparable to existing dominant terms (check `reward_breakdown` logs).

---

## Phase 4: Test

### Experiment Protocol

```yaml
# Record this BEFORE running the experiment
experiment:
  id: "RPE_YYYYMMDD_NNN"         # Auto-incrementing
  hypothesis: "<one sentence>"
  change: "<what exactly changed>"
  baseline: "<what to compare against>"
  environment: "<env id>"
  seeds: [42, 123, 456]           # Minimum 3 seeds
  steps: 2_000_000                # Short exploratory run
  metrics_to_watch:
    - episode_reward_mean
    - <behavior-specific metric>
```

### Running Quick Tests

```powershell
# Quick test â€” single run with rendering to visually verify behavior
uv run scripts/train.py --env vbot_navigation_section001 --render

# Full AutoML test with HP search
uv run starter_kit_schedule/scripts/automl.py `
    --mode stage `
    --budget-hours 4 `
    --hp-trials 3
```

### What Counts as "Enough" Testing

| Purpose | Steps | Seeds |
|---------|-------|-------|
| Quick sanity check | 500K | 1 |
| Hypothesis validation | 2M | 3 |
| Serious candidate | 5M | 3 |
| Pre-deployment | 10M+ | 5 |

---

## Phase 5: Evaluate

### Comparison Checklist

After a test run, answer these questions:

1. **Did the target behavior improve?** (Watch the policy, not just numbers)
2. **Did any other behavior degrade?** (Side effects)
3. **Is the improvement consistent across seeds?** (Not lucky variance)
4. **Is the reward curve healthy?** (Monotonic improvement, no collapse)
5. **Did training become slower?** (Some signals slow convergence)

### Decision Matrix

| Target improved? | Side effects? | Consistent? | Verdict |
|------------------|---------------|-------------|---------|
| Yes | None | Yes | **ADOPT** â€” Archive and integrate |
| Yes | Minor | Yes | **ITERATE** â€” Tune weight to reduce side effect |
| Yes | Major | Yes | **RETHINK** â€” Signal is right, mechanism needs redesign |
| Yes | Any | No | **INCONCLUSIVE** â€” More seeds or longer run |
| No | â€” | â€” | **REJECT** â€” Archive with notes, try different approach |

### Quantitative Evaluation

Use `starter_kit_schedule/scripts/analyze.py` for systematic comparison:

```powershell
# Compare experiments by reward metric
uv run starter_kit_schedule/scripts/analyze.py `
    --metric episode_reward_mean `
    --group-by reward_config

# Visual comparison via subagent
copilot --model gpt-4.1 --allow-all -p "Compare reward curves in runs/<exp_a>/ vs runs/<exp_b>/. Which converged faster? Which is more stable?" -s
```

---

## Phase 6: Archive

### Why Archive Everything

Every experiment result â€” positive or negative â€” is valuable. Negative results prevent re-trying failed ideas. Positive results enable reuse and combination.

### Reward Library Location

All reward/penalty instances are archived in:

```
starter_kit_schedule/reward_library/
â”œâ”€â”€ README.md                      # Library index and conventions
â”œâ”€â”€ components/                    # Individual reward/penalty definitions
â”‚   â””â”€â”€ <component_name>.yaml      # One per reward idea
â”œâ”€â”€ configs/                       # Complete reward configurations
â”‚   â””â”€â”€ <config_name>.yaml         # Tested combinations that work
â””â”€â”€ rejected/                      # Ideas that didn't work
    â””â”€â”€ <idea_name>.yaml           # With notes on why
```

### Component Archive Format

```yaml
# starter_kit_schedule/reward_library/components/<name>.yaml
name: "<descriptive name>"
type: "reward" | "penalty"
category: "navigation" | "stability" | "efficiency" | "terrain" | "gait" | "safety"
status: "tested" | "promising" | "rejected" | "untested"

description: "<what this signal does>"
hypothesis: "<why it should help>"
mechanism: "<mathematical formulation or pseudocode>"

weight_range:
  tested: [<min>, <max>]
  recommended: <value>

terrain_applicability:
  - flat
  - stairs
  - waves
  - obstacles

experiments:
  - id: "RPE_20260206_001"
    result: "improved" | "degraded" | "neutral"
    notes: "<brief finding>"

implementation:
  file: "<path to _compute_reward if custom code needed>"
  config_key: "<key in reward_scales dict>"
  requires_code_change: true | false
```

### Config Archive Format

```yaml
# starter_kit_schedule/reward_library/configs/<name>.yaml
name: "<config name>"
description: "<what this config is optimized for>"
environment: "<env id>"
date_tested: "YYYY-MM-DD"

reward_scales:
  position_tracking: 2.0
  heading_tracking: 1.0
  # ... all weights
  termination: -200.0

performance:
  episode_reward_mean: <value>
  success_rate: <value>
  notes: "<qualitative assessment>"

based_on: "<parent config name, if iterative>"
```

### Archiving Commands

```powershell
# After an experiment, archive the finding
# (manual for now â€” create the YAML by hand or instruct Copilot)

# List what's in the library
Get-ChildItem starter_kit_schedule/reward_library/components/ -Name
Get-ChildItem starter_kit_schedule/reward_library/configs/ -Name
Get-ChildItem starter_kit_schedule/reward_library/rejected/ -Name
```

---

## Anti-Patterns to Avoid

| Anti-Pattern | Why It Fails | Instead |
|--------------|-------------|---------|
| Changing 3+ rewards at once | Cannot attribute outcomes | One variable per cycle |
| Copying rewards from papers | Context differs | Use as hypothesis, test locally |
| Only watching reward curves | High reward â‰  good behavior | Always watch policy visually |
| Discarding failed experiments | Wastes future effort | Archive in `rejected/` |
| Tuning weights without diagnosis | Blind search | Diagnose behavior first |
| Adding rewards without removing | Reward bloat, slow training | Ablate unneeded components |
| Assuming terrain-agnostic rewards | Different terrains need different signals | Test per-terrain |
| **Unconditional per-step bonuses** | **Robot learns to survive, not navigate. alive_bonus Ã— max_steps >> goal reward** | **Make per-step bonuses conditional (e.g., only before reaching goal)** |
| **Goal reward too small vs per-step** | **arrival_bonus=15 vs alive_bonusÃ—3800â‰ˆ1900** | **arrival_bonus must dominate: set to â‰¥ alive Ã— typical_episode_len / 3** |
| **Trusting reward curves alone** | **"Reward=7.6" looked great, but robot was lazy** | **Always verify with distance, reached%, episode length metrics** |

---

## ğŸ”´ Case Study: The Lazy Robot (Reward Hacking)

This case study documents a critical reward hacking failure discovered during Round 1 full training, and the three-pronged fix.

### Timeline

1. **Round 1 AutoML** (15 trials, 5M steps): Best trial achieved reward=6.75, reached=6.45%. Looked promising.
2. **50M full training**: Early metrics excellent â€” distance 7.6â†’0.65m, arrival_total=18.76 (~90% reach rate).
3. **At step 24320 (~30min)**: Robot became lazy. Distance went UP (0.65â†’1.65m), reached% dropped (2.93â†’0.30%), episode length near max (3777/4000).
4. **Root cause**: `alive_bonus(0.5) Ã— ~3800 steps â‰ˆ 1900` dwarfed `arrival_bonus = 15`. The robot discovered standing still in a safe spot was the optimal strategy.

### Detection Signals

| Metric | Healthy | Lazy Robot |
|--------|---------|------------|
| Distance to target | Decreasing â†’ 0 | Decreasing then **increasing** |
| Reached % | Increasing | Increasing then **dropping to ~0** |
| Episode length | Moderate (500-2000) | **Near max_steps (3800+)** |
| Reward | Increasing | **Still looks good** (alive_bonus accumulates!) |
| Arrival bonus (TensorBoard) | Increasing | **Dropping toward 0** |

### The Fix: Anti-Laziness Trifecta

#### 1. Conditional alive_bonus
```python
# BEFORE (exploitable):
alive_bonus = scales["alive_bonus"]  # 0.5 every step, unconditionally

# AFTER (fixed):
alive_bonus = np.where(ever_reached, 0.0, scales["alive_bonus"])
# Once robot reaches target, alive bonus stops â†’ no "stand around" exploit
```

#### 2. Time decay on navigation rewards
```python
time_decay = np.clip(1.0 - 0.5 * env_steps / max_episode_steps, 0.5, 1.0)
# Step 0: time_decay = 1.0 (full reward)
# Step 2000 (half episode): time_decay = 0.75
# Step 4000 (max): time_decay = 0.5
# Creates URGENCY â€” reach the goal early for maximum reward
# NOTE: Penalties are NOT multiplied by time_decay (stay full strength)
```

#### 3. Massive arrival_bonus
```python
# BEFORE: arrival_bonus = 15 (trivially beaten by alive_bonus accumulation)
# AFTER: arrival_bonus = 50 (searched in range [20, 100])
# Rule of thumb: arrival_bonus > alive_bonus Ã— max_steps / 4
```

### Lesson

**Always audit the reward budget.** Compute: what is the maximum reward achievable by the desired behavior vs. by the degenerate behavior? If they're close, the agent WILL find the exploit given enough training time.

---

## Integration with Other Skills

This skill (methodology) connects to other skills (execution) as follows:

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  reward-penalty-engineering  â”‚
                    â”‚     (THIS SKILL)             â”‚
                    â”‚  HOW to explore rewards      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â–¼                  â–¼                       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ quadruped-      â”‚ â”‚ curriculum-      â”‚ â”‚ hyperparameter-     â”‚
  â”‚ competition-    â”‚ â”‚ learning         â”‚ â”‚ optimization        â”‚
  â”‚ tutor           â”‚ â”‚                  â”‚ â”‚                     â”‚
  â”‚                 â”‚ â”‚ Stage-specific   â”‚ â”‚ Automated reward    â”‚
  â”‚ Reward code     â”‚ â”‚ reward overrides â”‚ â”‚ weight search       â”‚
  â”‚ examples,       â”‚ â”‚ and progression  â”‚ â”‚ (grid/bayesian)     â”‚
  â”‚ competition     â”‚ â”‚                  â”‚ â”‚                     â”‚
  â”‚ score rules     â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚                       â”‚
           â”‚            â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
           â”‚            â”‚         training-campaign          â”‚
           â”‚            â”‚  Execute experiments, log results  â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                                    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚      subagent-copilot-cli          â”‚
                        â”‚  Visual diagnosis of reward effectsâ”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Archive storage: starter_kit_schedule/reward_library/
  Component reference: starter_kit_schedule/templates/reward_config_template.yaml
```

### Delegation Guide

| "I need to..." | Delegate to |
|-----------------|-------------|
| See what reward components exist and their scale ranges | Read `reward_config_template.yaml` |
| Find reward code for specific terrain challenges | Read `quadruped-competition-tutor` skill |
| Set up reward overrides per curriculum stage | Read `curriculum-learning` skill |
| Run automated reward weight search | Read `hyperparameter-optimization` skill |
| Execute and monitor a reward experiment | Read `training-campaign` skill |
| Visually inspect what a reward change did | Read `subagent-copilot-cli` skill |
| Understand competition scoring to align rewards | Read `quadruped-competition-tutor` skill |
| Modify the VBot MJCF model | Read `mjcf-xml-reasoning` skill |

---

## Summary: Exploration Philosophy

1. **Diagnose before prescribing** â€” Watch the policy, identify the behavioral gap
2. **One variable per experiment** â€” Isolate cause and effect
3. **Test short, test often** â€” 2M step runs across 3 seeds reveal more than one 20M run
4. **Archive everything** â€” Positive results reuse, negative results prevent repetition
5. **Methodology over recipes** â€” The process of finding good rewards matters more than any specific reward
6. **Library over memory** â€” Store tried components in `reward_library/`, not in your head
7. **Competition â‰  training** â€” Verify that training rewards actually improve competition score
